import os
import json
from typing import (
    Annotated,
    Sequence,
    TypedDict,
)
from langchain_core.messages import BaseMessage
from langgraph.graph.message import add_messages
from langchain_core.messages import ToolMessage, HumanMessage, RemoveMessage
from langchain_core.runnables import RunnableConfig
from langgraph.config import get_stream_writer
from langchain_core.tools import BaseTool, ToolException
from langgraph.graph import StateGraph, END
from langgraph.graph.state import CompiledStateGraph
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.vectorstores import InMemoryVectorStore, VectorStoreRetriever
from langchain.document_loaders import DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.embeddings import Embeddings

from langgraph.runtime import Runtime
from typing_extensions import Literal
import langgraph.types 
from dataclasses import dataclass
from typing import Dict

@dataclass
class Context:
    """Hold the context of what the user is seeing (e.g cluster, namespace, ...)."""
    context: Dict[str, str]

class AgentState(TypedDict):
    """The state of the agent."""
    messages: Annotated[Sequence[BaseMessage], add_messages]
    summary: str

def init_rag_rancher(embedding_model: Embeddings) -> VectorStoreRetriever:
    """
    Creates a retriever for Rancher documentation using RAG (Retrieval-Augmented Generation).

    Args:
        embedding_model: The embedding model to use for creating document embeddings.

    Returns:
        A VectorStoreRetriever that can be used to fetch relevant documents.
    """
    # test if `/rancher_docs` exists and contains files
    doc_path = os.environ.get("DOCS_PATH", "/rancher_docs")
    if not os.path.exists(doc_path) or not os.listdir(doc_path):
        raise FileNotFoundError("The directory /rancher_docs does not exist or is empty.")
    # load all markdown files in the directory
    loader = DirectoryLoader(doc_path, glob="**/*.md")
    docs = loader.load()
    print(f"â†’ {len(docs)} raw documents loaded")
    # 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,  # chunk size (characters)
        chunk_overlap=200,  # chunk overlap (characters)
        add_start_index=True,  # track index in original document
    )
    all_splits = text_splitter.split_documents(docs)
    # Initialize the vector store
    vector_store = InMemoryVectorStore(embedding_model)  
    vector_store.add_documents(documents=all_splits)
    retriever = vector_store.as_retriever(search_kwargs={"k": 6})
    return retriever
    
def create_k8s_agent(llm: BaseChatModel, tools: list[BaseTool], system_prompt: str) -> CompiledStateGraph:
    """
    Creates a LangGraph agent capable of interacting with Kubernetes resources.
    
    Args:
        llm: The language model to use for the agent's decisions.
        tools: A list of tools the agent can use (e.g., to interact with K8s).
        system_prompt: The initial system-level instructions for the agent.
    
    Returns:
        A compiled LangGraph StateGraph ready to be invoked.
    """
    
    llm_with_tools = llm.bind_tools(tools)

    def should_interrupt(tool_call: any) -> str:
        """
        Checks if a user-facing interruption is needed before executing a tool call.
        
        Args:
            tool_call: The tool call generated by the LLM.
            
        Returns:
            A string with the interrupt message if a confirmation is required,
            otherwise an empty string.
        """

        if tool_call["name"] == "patchKubernetesResource":
            return f"The following patch: {tool_call['args']['patch']} will be applied in the {tool_call['args']['name']} {tool_call['args']['kind']} within the {tool_call['args']['cluster']} cluster. WARNING: This action will modify cluster resources. Do you want to proceed? (yes/no)"
        return ""
    
    async def tool_node(state: AgentState):
        """
        This node executes the tool calls requested by the agent.
        
        Args:
            state: The current state of the agent, containing messages.
            
        Returns:
            A dictionary to update the state with the results of the tool calls.
        """

        tools_by_name = {tool.name: tool for tool in tools}
        outputs = []
        for tool_call in state["messages"][-1].tool_calls:
            if interrupt_message := should_interrupt(tool_call):
                response = langgraph.types.interrupt(interrupt_message) 
                if response["response"] != "yes":
                    return {"messages": "the tool execution was cancelled by the user."}
            try:
                tool_result = await tools_by_name[tool_call["name"]].ainvoke(tool_call["args"])
                json_result = json.loads(tool_result)
                writer = get_stream_writer()  
                writer(f"<mcp-response>{json_result['uiContext']}</mcp-response>") 
                outputs.append(
                    ToolMessage(
                        content=json_result["llm"],
                        name=tool_call["name"],
                        tool_call_id=tool_call["id"])
                )
            except ToolException as e:
                return {"messages": str(e)}
        return {"messages": outputs}

    def call_model(
        state: AgentState,
        config: RunnableConfig,
        runtime: Runtime[Context]
    ):
        """
        This node invokes the language model to generate a response or tool calls.
        
        Args:
            state: The current state of the agent.
            config: Configuration for the runnable.
            runtime: The runtime context, used here to include default values.
        
        Returns:
            A dictionary to update the state with the LLM's response.
        """

        messages_to_send = [system_prompt]
        if len(runtime.context.context.items()) > 0:
            context_prompt = "Prioritize the following default values for tool parameters. You must use these exact values unless the user explicitly provides a different value for a specific parameter in their request: "
            for key, value in runtime.context.context.items():
                context_prompt += f"key={key}, value={value}\n"
            messages_to_send.append(context_prompt)
        response = llm_with_tools.invoke(messages_to_send + state["messages"], config)

        return {"messages": [response]}

    # Define the conditional edge that determines whether to continue or not
    def should_continue(state: AgentState):
        """
        Determines the next step in the workflow based on the last message from the LLM.
        
        Args:
            state: The current agent state.
            
        Returns:
            "end" if the last message is a final answer, "continue" if it's a tool call.
        """
        messages = state["messages"]
        last_message = messages[-1]
        if not last_message.tool_calls:
            return "end"
        else:
            return "continue"

    workflow = StateGraph(AgentState)

    workflow.add_node("agent", call_model)
    workflow.add_node("tools", tool_node)
    workflow.set_entry_point("agent")
    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "continue": "tools",
            "end": END,
        },
    )
    workflow.add_edge("tools", "agent")

    return workflow
