from typing import (
    Annotated,
    Sequence,
    TypedDict,
)
from langchain_core.messages import BaseMessage
from langgraph.graph.message import add_messages
from langchain_core.messages import ToolMessage
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import BaseTool, ToolException
from langgraph.graph import StateGraph, END
from langgraph.graph.state import CompiledStateGraph
from langchain_core.language_models.chat_models import BaseChatModel
from langgraph.runtime import Runtime
import langgraph.types 
from dataclasses import dataclass
from typing import Dict

@dataclass
class Context:
    """Hold the context of what the user is seeing (e.g cluster, namespace, ...)."""
    context: Dict[str, str]

class AgentState(TypedDict):
    """The state of the agent."""
    messages: Annotated[Sequence[BaseMessage], add_messages]


def create_k8s_agent(llm: BaseChatModel, tools: list[BaseTool], system_prompt: str) -> CompiledStateGraph:
    """
    Creates a LangGraph agent capable of interacting with Kubernetes resources.
    
    Args:
        llm: The language model to use for the agent's decisions.
        tools: A list of tools the agent can use (e.g., to interact with K8s).
        system_prompt: The initial system-level instructions for the agent.
    
    Returns:
        A compiled LangGraph StateGraph ready to be invoked.
    """

    llm_with_tools = llm.bind_tools(tools)

    def should_interrupt(tool_call: any) -> str:
        """
        Checks if a user-facing interruption is needed before executing a tool call.
        
        Args:
            tool_call: The tool call generated by the LLM.
            
        Returns:
            A string with the interrupt message if a confirmation is required,
            otherwise an empty string.
        """

        if tool_call["name"] == "patchKubernetesResource":
            return f"The following patch: {tool_call['args']['patch']} will be applied in the {tool_call['args']['name']} {tool_call['args']['kind']} within the {tool_call['args']['cluster']} cluster. WARNING: This action will modify cluster resources. Do you want to proceed? (yes/no)"
        return ""
    
    async def tool_node(state: AgentState):
        """
        This node executes the tool calls requested by the agent.
        
        Args:
            state: The current state of the agent, containing messages.
            
        Returns:
            A dictionary to update the state with the results of the tool calls.
        """

        tools_by_name = {tool.name: tool for tool in tools}
        outputs = []
        for tool_call in state["messages"][-1].tool_calls:
            if interrupt_message := should_interrupt(tool_call):
                response = langgraph.types.interrupt(interrupt_message) 
                if response["response"] != "yes":
                    return {"messages": "the tool execution was cancelled by the user."}
            try:
                tool_result = await tools_by_name[tool_call["name"]].ainvoke(tool_call["args"])
                outputs.append(
                    ToolMessage(
                        content=tool_result,
                        name=tool_call["name"],
                        tool_call_id=tool_call["id"])
                )
            except ToolException as e:
                return {"messages": str(e)}
        return {"messages": outputs}

    def call_model(
        state: AgentState,
        config: RunnableConfig,
        runtime: Runtime[Context]
    ):
        """
        This node invokes the language model to generate a response or tool calls.
        
        Args:
            state: The current state of the agent.
            config: Configuration for the runnable.
            runtime: The runtime context, used here to include default values.
        
        Returns:
            A dictionary to update the state with the LLM's response.
        """

        messages_to_send = [system_prompt]
        if len(runtime.context.context.items()) > 0:
            context_prompt = "Prioritize the following default values for tool parameters. You must use these exact values unless the user explicitly provides a different value for a specific parameter in their request: "
            for key, value in runtime.context.context.items():
                context_prompt += f"key={key}, value={value}\n"
            messages_to_send.append(context_prompt)
        response = llm_with_tools.invoke(messages_to_send + state["messages"], config)

        return {"messages": [response]}

    # Define the conditional edge that determines whether to continue or not
    def should_continue(state: AgentState):
        """
        Determines the next step in the workflow based on the last message from the LLM.
        
        Args:
            state: The current agent state.
            
        Returns:
            "end" if the last message is a final answer, "continue" if it's a tool call.
        """
        messages = state["messages"]
        last_message = messages[-1]
        if not last_message.tool_calls:
            return "end"
        else:
            return "continue"

    workflow = StateGraph(AgentState)

    workflow.add_node("agent", call_model)
    workflow.add_node("tools", tool_node)
    workflow.set_entry_point("agent")
    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "continue": "tools",
            "end": END,
        },
    )
    workflow.add_edge("tools", "agent")

    return workflow
